{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85335797-57c7-4a2a-b2e8-67fa83a5339a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0%   [24577/24577]\n"
     ]
    }
   ],
   "source": [
    "# Convert audio file to features (log mel mean, log mel max, log mel min, log mel delta mean, log mel delta max, log mel delta min )\n",
    "num_loaded = 0\n",
    "target_frames = 86  # ≈ 2 seconds with librosa defaults (sr=22050, hop_length=512)\n",
    "printProgressBar(0, len(audio_files), prefix='Progress:', suffix='Complete', length=50)\n",
    "input_features = np.empty((len(audio_files), 2), dtype=object)\n",
    "\n",
    "for afile in audio_files:\n",
    "    # Extract y = (the raw data), and sr = (integer value of sample rate)\n",
    "    y, sr = librosa.load(afile)\n",
    "    # Apply STFT\n",
    "    D = librosa.stft(y)\n",
    "    # Retreive Mel\n",
    "    S = librosa.feature.melspectrogram(y=y,\n",
    "                                       sr=sr,\n",
    "                                       n_mels=128 * 2,)\n",
    "    S_decible_mel = librosa.amplitude_to_db(S, ref=np.max)\n",
    "    # Extract Log Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    # Extract Delta Mel spectrogram\n",
    "    delta_log_mel_spectrogram = librosa.feature.delta(log_mel_spectrogram)\n",
    "    input_features[num_loaded, 0] = log_mel_spectrogram\n",
    "    input_features[num_loaded, 1] = delta_log_mel_spectrogram\n",
    "    # sample_features = np.stack([(log_mel_spectrogram.T, delta_log_mel_spectrogram.T)], axis=-1)\n",
    "    # input_features.append(sample_features)\n",
    "    num_loaded += 1\n",
    "    printProgressBar(\n",
    "        num_loaded,\n",
    "        len(audio_files),\n",
    "        prefix='Progress:',\n",
    "        suffix=f'  [{num_loaded}/{len(audio_files)}]',\n",
    "        length=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b064db2-5e08-48a2-9a4b-e1dd1afd061b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0%   [24577/24577]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     44\u001b[39m     num_loaded += \u001b[32m1\u001b[39m\n\u001b[32m     45\u001b[39m     printProgressBar(\n\u001b[32m     46\u001b[39m         num_loaded,\n\u001b[32m     47\u001b[39m         \u001b[38;5;28mlen\u001b[39m(audio_files),\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m         length=\u001b[32m50\u001b[39m\n\u001b[32m     51\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m input_tensor = \u001b[43mtorch\u001b[49m.tensor(np.array(input_features_list), dtype=torch.float32)\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinal tensor shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_tensor.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert audio file to features (log mel spectogram and log mel delta)\n",
    "num_loaded = 0\n",
    "target_frames = 86  # ≈ 2 seconds with librosa defaults (sr=22050, hop_length=512)\n",
    "printProgressBar(0, len(audio_files), prefix='Progress:', suffix='Complete', length=50)\n",
    "input_features_list = []\n",
    "\n",
    "for afile in audio_files:\n",
    "    # Extract y = (the raw data), and sr = (integer value of sample rate)\n",
    "    y, sr = librosa.load(afile)\n",
    "    \n",
    "    # Apply STFT\n",
    "    D = librosa.stft(y)\n",
    "    \n",
    "    # Retreive Mel\n",
    "    S = librosa.feature.melspectrogram(y=y,\n",
    "                                       sr=sr,\n",
    "                                       n_mels=128 * 2,)\n",
    "    S_decible_mel = librosa.amplitude_to_db(S, ref=np.max)\n",
    "    \n",
    "    # Extract Log Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Standardize the length using padding/truncation\n",
    "    # The shape is (n_mels, time_frames). We fix the second dimension.\n",
    "    fixed_log_mel = librosa.util.fix_length(log_mel_spectrogram, \n",
    "                                            size=target_frames, \n",
    "                                            axis=1, \n",
    "                                            constant_values=0 # Pad with zeros if shorter\n",
    "                                            )\n",
    "    \n",
    "    # Extract Delta Mel spectrogram\n",
    "    delta_log_mel_spectrogram = librosa.feature.delta(log_mel_spectrogram)\n",
    "    \n",
    "    # Standardize the delta spectrogram length too\n",
    "    fixed_delta_log_mel = librosa.util.fix_length(delta_log_mel_spectrogram, \n",
    "                                                  size=target_frames, \n",
    "                                                  axis=1,\n",
    "                                                  constant_values=0\n",
    "                                                 )\n",
    "\n",
    "    sample_features = np.stack([fixed_log_mel, fixed_delta_log_mel], axis=0)\n",
    "    input_features_list.append(sample_features)\n",
    "    num_loaded += 1\n",
    "    printProgressBar(\n",
    "        num_loaded,\n",
    "        len(audio_files),\n",
    "        prefix='Progress:',\n",
    "        suffix=f'  [{num_loaded}/{len(audio_files)}]',\n",
    "        length=50\n",
    "    )\n",
    "input_tensor = torch.tensor(np.array(input_features_list), dtype=torch.float32)\n",
    "print(f\"Final tensor shape: {input_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b11ea85-af48-4e7a-827d-2a733dc4a825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final tensor shape: torch.Size([24577, 2, 128, 86])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input_tensor = torch.tensor(np.array(input_features_list), dtype=torch.float32)\n",
    "print(f\"Final tensor shape: {input_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7168ab6-ffd4-4b06-8c1d-1cc8695dab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(input_tensor, 'input.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01607fe7-8db2-475c-901b-b257088f32ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24577 files from dataset.\n",
      "Progress: |██████████████████████████████████████████████████| 100.0%   [24577/24577]\n",
      "Final tensor shape: torch.Size([24577, 6])\n",
      "Wrote numpy arrays to file for training in /Users/clinvil/Programs/ToneAI/labs/DNN\n",
      "Completed preprocessing!\n"
     ]
    }
   ],
   "source": [
    "from progressbar import printProgressBar\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from itertools import cycle\n",
    "\n",
    "sns.set_theme(style=\"white\", palette=None)\n",
    "color_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "color_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])\n",
    "\n",
    "\n",
    "# File Paths\n",
    "\n",
    "# Get the directory of the current script\n",
    "script_dir = os.getcwd()\n",
    "# Other Project Paths\n",
    "root_dir = os.path.join(script_dir, '..', '..')\n",
    "training_data_dir = os.path.join(root_dir, 'training data')\n",
    "categories_dir = os.path.join(training_data_dir, 'categories')\n",
    "cremad_dir = os.path.join(training_data_dir, 'files', 'CREMA-D')\n",
    "emogator_dir = os.path.join(training_data_dir, 'files', 'Emogator', 'data', 'mp3')\n",
    "datasheeet_path = os.path.join(categories_dir, 'data.xlsx')\n",
    "\n",
    "\n",
    "# Read Data\n",
    "audio_files = []\n",
    "data_raw_df = pd.read_excel(datasheeet_path)\n",
    "headers = data_raw_df.columns.values.tolist()\n",
    "data_raw_noheaders_df = data_raw_df.values\n",
    "data_df = pd.DataFrame(data_raw_noheaders_df)\n",
    "\n",
    "# Extract Targets\n",
    "emotion_target_categories = headers[3:11]\n",
    "intensity_target_categories = headers[12:]\n",
    "selected_emotion_targets_df = data_df.iloc[:, [i for i in range(3, 11)]]\n",
    "selected_intensity_targets_df = data_df.iloc[:, [i for i in range(12, 15)]]\n",
    "emotion_targets = selected_emotion_targets_df.to_numpy()\n",
    "intensity_targets = selected_intensity_targets_df.to_numpy()\n",
    "\n",
    "# Load the audio files\n",
    "datasets = data_raw_df['Dataset'].values\n",
    "files = data_raw_df['File'].values\n",
    "num_loaded = 0\n",
    "for dataset, file in zip(datasets, files):\n",
    "    if dataset == 'CREMA-D':\n",
    "        file_path = os.path.join(cremad_dir, file)\n",
    "        audio_files.append(file_path)\n",
    "        num_loaded += 1\n",
    "    elif dataset == 'EmoGator':\n",
    "        file_path = os.path.join(emogator_dir, file)\n",
    "        audio_files.append(file_path)\n",
    "        num_loaded += 1\n",
    "print(f'Loaded {num_loaded} files from dataset.')\n",
    "\n",
    "# Convert audio file to features (log mel spectogram and log mel delta)\n",
    "num_loaded = 0\n",
    "printProgressBar(0, len(audio_files), prefix='Progress:', suffix='Complete', length=50)\n",
    "input_features_list = []\n",
    "\n",
    "for afile in audio_files:\n",
    "    # Extract y = (the raw data), and sr = (integer value of sample rate)\n",
    "    y, sr = librosa.load(afile)\n",
    "    \n",
    "    \n",
    "    # Apply STFT\n",
    "    D = librosa.stft(y)\n",
    "    \n",
    "    # Retreive Mel\n",
    "    S = librosa.feature.melspectrogram(y=y,\n",
    "                                       sr=sr,\n",
    "                                       n_mels=128 * 2,)\n",
    "    S_decible_mel = librosa.amplitude_to_db(S, ref=np.max)\n",
    "    \n",
    "    # Extract Log Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    mean_mel_log = float(np.mean(log_mel_spectrogram))\n",
    "    min_mel_log = float(np.min(log_mel_spectrogram))\n",
    "    max_mel_log = float(np.max(log_mel_spectrogram))\n",
    "    \n",
    "    # Extract Delta Mel spectrogram\n",
    "    delta_log_mel_spectrogram = librosa.feature.delta(log_mel_spectrogram)\n",
    "    mean_delta_mel_log = float(np.mean(delta_log_mel_spectrogram))\n",
    "    min_delta_mel_log = float(np.min(delta_log_mel_spectrogram))\n",
    "    max_delta_mel_log = float(np.max(delta_log_mel_spectrogram))\n",
    "    \n",
    "    sample_features = np.array(\n",
    "        [\n",
    "            mean_mel_log,\n",
    "            mean_delta_mel_log,\n",
    "            max_mel_log,\n",
    "            max_delta_mel_log,\n",
    "            min_mel_log,\n",
    "            min_delta_mel_log,\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    input_features_list.append(sample_features)\n",
    "    num_loaded += 1\n",
    "    printProgressBar(\n",
    "        num_loaded,\n",
    "        len(audio_files),\n",
    "        prefix='Progress:',\n",
    "        suffix=f'  [{num_loaded}/{len(audio_files)}]',\n",
    "        length=50\n",
    "    )\n",
    "\n",
    "# Save file to avoid preprocessing more\n",
    "# np.savetxt('input_features.json', input_features, delimiter=',', fmt='%d', comments='')\n",
    "np.savetxt('emotion_targets.csv', emotion_targets, delimiter=',', fmt='%d', comments='')\n",
    "np.savetxt('intensity_targets.csv', intensity_targets, delimiter=',', fmt='%d', comments='')\n",
    "np.savetxt('intensity_targets.csv', np.array(input_features_list), delimiter=',', fmt='%d', comments='')\n",
    "# input_tensor = torch.tensor(np.array(input_features_list), dtype=torch.float32)\n",
    "# torch.save(input_tensor, 'input.pt')\n",
    "print(f\"Final tensor shape: {input_tensor.shape}\")\n",
    "print(f'Wrote numpy arrays to file for training in {script_dir}')\n",
    "print('Completed preprocessing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca89764d-c9d2-4781-aaf1-a79b9f31d52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.5341328e+01,  6.1176196e-02,  0.0000000e+00,  9.6657677e+00,\n",
       "       -8.0000000e+01, -6.0179114e+00], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "625d4a49-a718-4ab3-ab8b-002e6678836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(input_tensor, 'input.pt')\n",
    "np.savetxt('intensity_targets.csv', intensity_targets, delimiter=',', fmt='%d', comments='')\n",
    "np.savetxt('input.csv', np.array(input_features_list), delimiter=',', fmt='%d', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f90f735c-beab-498e-94a8-7dffb5eb2686",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     78\u001b[39m outputs = model(inputs)\n\u001b[32m     79\u001b[39m loss = criterion(outputs, targets)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m optimizer.step()\n\u001b[32m     83\u001b[39m running_train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/ToneAI/.venv/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/ToneAI/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/ToneAI/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Found dtype Double but expected Float"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Variables\n",
    "train_slice = .75\n",
    "validation_slice = .15\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Datasets\n",
    "script_dir = os.getcwd()\n",
    "input_features_path = os.path.join(script_dir, 'input.pt')\n",
    "input_features = torch.load(input_features_path).float()\n",
    "emotion_targets_np = np.loadtxt(os.path.join(script_dir, 'emotion_targets.csv'), delimiter=',') \n",
    "emotion_targets = torch.from_numpy(emotion_targets_np)\n",
    "dataset = TensorDataset(input_features, emotion_targets)\n",
    "\n",
    "# Sanity check: same number of samples\n",
    "assert input_features.shape[0] == emotion_targets.shape[0], \\\n",
    "    f\"Input/target size mismatch: {input_features.shape[0]} vs {emotion_targets.shape[0]}\"\n",
    "\n",
    "# Split model sets\n",
    "train_size = int(train_slice * len(dataset))\n",
    "val_size = int(validation_slice * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Model\n",
    "# --------------------\n",
    "input_dim = input_features.shape[1]      # e.g. 6 if you used [mean, max, min, ...]\n",
    "output_dim = emotion_targets.shape[1]    # number of emotion targets\n",
    "\n",
    "class SimpleDNN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(SimpleDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleDNN(input_dim, output_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Optional: use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# --------------------\n",
    "# Training loop\n",
    "# --------------------\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, targets).item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# --------------------\n",
    "# Test evaluation\n",
    "# --------------------\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        test_loss += criterion(outputs, targets).item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f'Test Loss: {avg_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1d9a3f1-e809-44bc-9647-629143768a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 101)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features[1,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd24d6c2-c85c-4333-9d0f-98d19ebd0210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 101)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features[1,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80bc60b0-0117-452a-b49b-1b96714652c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote numpy arrays to file for training in /Users/clinvil/Programs/ToneAI/labs/DNN\n",
      "Completed preprocessing!\n"
     ]
    }
   ],
   "source": [
    "# Save file to avoid preprocessing more\n",
    "# np.savetxt('input_features.json', input_features, delimiter=',', fmt='%d', comments='')\n",
    "np.savetxt('emotion_targets.csv', emotion_targets, delimiter=',', fmt='%d', comments='')\n",
    "np.savetxt('intensity_targets.csv', intensity_targets, delimiter=',', fmt='%d', comments='')\n",
    "np.save('input_features.npy', input_features)\n",
    "print(f'Wrote numpy arrays to file for training in {script_dir}')\n",
    "print('Completed preprocessing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c8ae08d-0dae-4658-9ba3-8bf95abb3a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_inputs = np.load(os.path.join(script_dir, 'input_features.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7eaffa8-f86d-404b-adc9-465f14c2c072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24577, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59961b7a-eee8-42ee-879f-4e1cc10fce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_targets = np.loadtxt(os.path.join(script_dir, 'emotion_targets.csv'), delimiter=',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bbbca490-ee15-4106-b4af-4706e40845a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24577, 8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    emotion_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "511bb301-09c1-48d8-aec9-bd51030e8f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1764698e+00,  2.1764698e+00,  2.1764698e+00, ...,\n",
       "        -7.8147256e-01, -7.8147256e-01, -7.8147256e-01],\n",
       "       [ 3.3123832e+00,  3.3123832e+00,  3.3123832e+00, ...,\n",
       "        -2.8086445e-01, -2.8086445e-01, -2.8086445e-01],\n",
       "       [ 3.4211106e+00,  3.4211106e+00,  3.4211106e+00, ...,\n",
       "        -4.6701780e-01, -4.6701780e-01, -4.6701780e-01],\n",
       "       ...,\n",
       "       [ 3.8968581e-15,  3.8968581e-15,  3.8968581e-15, ...,\n",
       "         3.8968581e-15,  3.8968581e-15,  3.8968581e-15],\n",
       "       [ 3.8968581e-15,  3.8968581e-15,  3.8968581e-15, ...,\n",
       "         3.8968581e-15,  3.8968581e-15,  3.8968581e-15],\n",
       "       [ 3.8968581e-15,  3.8968581e-15,  3.8968581e-15, ...,\n",
       "         3.8968581e-15,  3.8968581e-15,  3.8968581e-15]],\n",
       "      shape=(128, 99), dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_inputs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55d1a416-8ac7-4741-ab93-235ad3dbabf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51da453b-d00a-46ae-9b03-f1531b1ff5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0cf2caa-3b97-4505-bc9d-15ef826d736c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9146 - loss: 0.2969\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9572 - loss: 0.1477\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9670 - loss: 0.1105\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9730 - loss: 0.0900\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9755 - loss: 0.0787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1772e0f50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f6a5c4f-d2a5-4e83-858f-5fa70286a4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - 801us/step - accuracy: 0.9762 - loss: 0.0778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0778232142329216, 0.9761999845504761]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e18242ad-5259-47d7-b0b7-9e759c7e8a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
       "array([[1.4479136e-06, 1.3826105e-08, 4.1731579e-05, 7.1064045e-04,\n",
       "        1.7275116e-10, 4.9821705e-07, 3.8129198e-11, 9.9921727e-01,\n",
       "        6.4445530e-06, 2.2055814e-05],\n",
       "       [2.3529320e-07, 1.9199195e-04, 9.9979466e-01, 6.9858806e-06,\n",
       "        2.1186967e-16, 2.4818557e-06, 1.3316050e-08, 1.1141943e-16,\n",
       "        3.6092849e-06, 9.0926878e-15],\n",
       "       [6.2030398e-07, 9.9941486e-01, 1.2275181e-04, 1.3997315e-05,\n",
       "        6.5566099e-05, 1.1564008e-06, 1.6209198e-05, 7.2746698e-05,\n",
       "        2.9184890e-04, 1.9371008e-07],\n",
       "       [9.9989861e-01, 6.3130554e-09, 1.0407096e-05, 1.6868885e-08,\n",
       "        3.0503568e-08, 3.4601075e-07, 2.6689897e-05, 1.0980951e-06,\n",
       "        5.1752711e-08, 6.2811443e-05],\n",
       "       [8.1732342e-06, 6.6827849e-11, 9.6839040e-06, 1.3209448e-08,\n",
       "        9.9593085e-01, 1.0363929e-07, 4.7587181e-07, 1.8974736e-04,\n",
       "        1.0322190e-06, 3.8598701e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "probability_model(x_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ca457b8-538b-483b-b923-8145bface919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotion_target_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff34138c-b930-4130-a37c-9500a13f33a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24577 files from dataset.\n",
      "Progress: |██████████████████████████████████████████████████| 100.0%   [24577/24577]\n",
      "Final EMOTION tensor shape: torch.Size([24577, 11008])\n",
      "Final INTENSITY tensor shape: torch.Size([24577, 11008])\n",
      "Wrote numpy arrays to file for training in /Users/clinvil/Programs/ToneAI/labs/DNN\n",
      "Completed preprocessing!\n"
     ]
    }
   ],
   "source": [
    "from progressbar import printProgressBar\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from itertools import cycle\n",
    "\n",
    "sns.set_theme(style=\"white\", palette=None)\n",
    "color_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "color_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])\n",
    "\n",
    "\n",
    "# File Paths\n",
    "\n",
    "# Get the directory of the current script\n",
    "script_dir = os.getcwd()\n",
    "# Other Project Paths\n",
    "root_dir = os.path.join(script_dir, '..', '..')\n",
    "training_data_dir = os.path.join(root_dir, 'training data')\n",
    "categories_dir = os.path.join(training_data_dir, 'categories')\n",
    "cremad_dir = os.path.join(training_data_dir, 'files', 'CREMA-D')\n",
    "emogator_dir = os.path.join(training_data_dir, 'files', 'Emogator', 'data', 'mp3')\n",
    "datasheeet_path = os.path.join(categories_dir, 'data.xlsx')\n",
    "\n",
    "\n",
    "# Read Data\n",
    "audio_files = []\n",
    "data_raw_df = pd.read_excel(datasheeet_path)\n",
    "headers = data_raw_df.columns.values.tolist()\n",
    "data_raw_noheaders_df = data_raw_df.values\n",
    "data_df = pd.DataFrame(data_raw_noheaders_df)\n",
    "\n",
    "# Extract Targets\n",
    "emotion_target_categories = headers[3:11]\n",
    "intensity_target_categories = headers[12:]\n",
    "selected_emotion_targets_df = data_df.iloc[:, [i for i in range(3, 11)]]\n",
    "selected_intensity_targets_df = data_df.iloc[:, [i for i in range(12, 15)]]\n",
    "emotion_targets = selected_emotion_targets_df.to_numpy()\n",
    "intensity_targets = selected_intensity_targets_df.to_numpy()\n",
    "\n",
    "# Load the audio files\n",
    "datasets = data_raw_df['Dataset'].values\n",
    "files = data_raw_df['File'].values\n",
    "num_loaded = 0\n",
    "for dataset, file in zip(datasets, files):\n",
    "    if dataset == 'CREMA-D':\n",
    "        file_path = os.path.join(cremad_dir, file)\n",
    "        audio_files.append(file_path)\n",
    "        num_loaded += 1\n",
    "    elif dataset == 'EmoGator':\n",
    "        file_path = os.path.join(emogator_dir, file)\n",
    "        audio_files.append(file_path)\n",
    "        num_loaded += 1\n",
    "print(f'Loaded {num_loaded} files from dataset.')\n",
    "\n",
    "# Convert audio file to features (log mel spectogram and log mel delta)\n",
    "num_loaded = 0\n",
    "target_frames = 86  # ≈ 2 seconds with librosa defaults (sr=22050, hop_length=512)\n",
    "printProgressBar(0, len(audio_files), prefix='Progress:', suffix='Complete', length=50)\n",
    "emotion_input_features_list = []\n",
    "intensity_input_features_list = []\n",
    "\n",
    "for afile in audio_files:\n",
    "    # Extract y = (the raw data), and sr = (integer value of sample rate)\n",
    "    y, sr = librosa.load(afile)\n",
    "    \n",
    "    # Apply STFT\n",
    "    D = librosa.stft(y)\n",
    "    \n",
    "    # Retreive Mel\n",
    "    S = librosa.feature.melspectrogram(y=y,\n",
    "                                       sr=sr,\n",
    "                                       n_mels=128 * 2,)\n",
    "    S_decible_mel = librosa.amplitude_to_db(S, ref=np.max)\n",
    "    \n",
    "    # Extract Log Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Standardize the length using padding/truncation #? The shape is (n_mels, time_frames) #?\n",
    "    fixed_log_mel = librosa.util.fix_length(log_mel_spectrogram, \n",
    "                                            size=target_frames, \n",
    "                                            axis=1, \n",
    "                                            constant_values=0)\n",
    "    \n",
    "    # Build input for emotion model\n",
    "    emotion_sample_features = fixed_log_mel.flatten(order='C')\n",
    "    emotion_input_features_list.append(emotion_sample_features)\n",
    "\n",
    "    \n",
    "    # Extract Delta Mel spectrogram\n",
    "    delta_log_mel_spectrogram = librosa.feature.delta(log_mel_spectrogram)\n",
    "    \n",
    "    # Standardize the delta spectrogram length too\n",
    "    fixed_delta_log_mel = librosa.util.fix_length(delta_log_mel_spectrogram, \n",
    "                                                  size=target_frames, \n",
    "                                                  axis=1,\n",
    "                                                  constant_values=0)\n",
    "\n",
    "    # Build input for intensity model\n",
    "    intensity_sample_features = fixed_delta_log_mel.flatten(order='C')\n",
    "    intensity_input_features_list.append(intensity_sample_features)\n",
    "    \n",
    "    num_loaded += 1\n",
    "    printProgressBar(\n",
    "        num_loaded,\n",
    "        len(audio_files),\n",
    "        prefix='Progress:',\n",
    "        suffix=f'  [{num_loaded}/{len(audio_files)}]',\n",
    "        length=50\n",
    "    )\n",
    "\n",
    "# Save file to avoid preprocessing more\n",
    "# np.savetxt('input_features.json', input_features, delimiter=',', fmt='%d', comments='')\n",
    "np.savetxt('emotion_targets.csv', emotion_targets, delimiter=',', fmt='%d', comments='')\n",
    "np.savetxt('intensity_targets.csv', intensity_targets, delimiter=',', fmt='%d', comments='')\n",
    "emotion_input_tensor = torch.tensor(np.array(emotion_input_features_list), dtype=torch.double)\n",
    "intensity_input_tensor = torch.tensor(np.array(intensity_input_features_list), dtype=torch.double)\n",
    "print(f\"Final EMOTION tensor shape: {emotion_input_tensor.shape}\")\n",
    "print(f\"Final INTENSITY tensor shape: {intensity_input_tensor.shape}\")\n",
    "print(f'Wrote numpy arrays to file for training in {script_dir}')\n",
    "print('Completed preprocessing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2c279355-f89f-4053-b758-2819bbee91b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24577, 11008])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "954cfb9e-69e8-4830-a9a3-db320d4fc217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24577, 11008])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intensity_input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3b6d3f6-11cc-43af-8f54-ceea7b43a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(emotion_input_tensor, 'model1_input.pt')\n",
    "torch.save(intensity_input_tensor, 'model2_input.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tone AI",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
