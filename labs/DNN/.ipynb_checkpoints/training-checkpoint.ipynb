{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ef2f36-ff33-4044-ab55-8ede356d1b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True cuda\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Check pytorch version\n",
    "torch.__version__\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Check and determine device type\n",
    "if torch.cuda.is_available():\n",
    "    gpuAvailable = True\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    gpuAvailable = True\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    gpuAvailable = False\n",
    "    device = torch.device(\"cpu\")\n",
    "print(gpuAvailable, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ad2d8f7-a7c5-4aef-8b3c-f1009a18350d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19661, 11008]) torch.Size([19661, 8])\n",
      "torch.Size([4916, 11008]) torch.Size([4916, 8])\n"
     ]
    }
   ],
   "source": [
    "# Set dataset up\n",
    "curdir = os.getcwd()\n",
    "X = torch.load(os.path.join(curdir, 'model1_input.pt')).to(torch.float32)\n",
    "y = torch.tensor(np.loadtxt(os.path.join(curdir, 'emotion_targets.csv'), delimiter=','), dtype=torch.float32)\n",
    "\n",
    "# Dataset variables\n",
    "train_slice = .80\n",
    "test_slice = .20\n",
    "\n",
    "# Setup training, testing, and validation sets\n",
    "# skf = StratifiedKFold(n_splits=a3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_slice, random_state=SEED, shuffle=True)\n",
    "# validation_slice = X_test.shape[0]/X_train.shape[0]\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_slice, random_state=SEED, shuffle=True)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd44d2f8-8209-4bff-8630-a166420b056d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss: 30.36344%, Acc: 13.05% | Test Loss: 29.53321%, Test Acc: 18.49% | F1-Score: 0.18\n",
      "Epoch: 20 | Loss: 6.76827%, Acc: 9.71% | Test Loss: 11.15126%, Test Acc: 18.49% | F1-Score: 0.18\n",
      "Epoch: 30 | Loss: 3.30567%, Acc: 22.48% | Test Loss: 3.37156%, Test Acc: 16.62% | F1-Score: 0.17\n",
      "Epoch: 40 | Loss: 2.08158%, Acc: 22.51% | Test Loss: 1.99462%, Test Acc: 21.66% | F1-Score: 0.22\n",
      "Epoch: 50 | Loss: 1.88055%, Acc: 20.72% | Test Loss: 1.86157%, Test Acc: 22.64% | F1-Score: 0.23\n",
      "Epoch: 60 | Loss: 1.79214%, Acc: 28.60% | Test Loss: 1.78091%, Test Acc: 27.18% | F1-Score: 0.27\n",
      "Epoch: 70 | Loss: 1.74824%, Acc: 30.52% | Test Loss: 1.75009%, Test Acc: 29.60% | F1-Score: 0.30\n",
      "Epoch: 80 | Loss: 1.72531%, Acc: 31.81% | Test Loss: 1.72885%, Test Acc: 30.13% | F1-Score: 0.30\n",
      "Epoch: 90 | Loss: 1.71043%, Acc: 32.21% | Test Loss: 1.71399%, Test Acc: 30.86% | F1-Score: 0.31\n",
      "Epoch: 100 | Loss: 1.69823%, Acc: 33.17% | Test Loss: 1.70418%, Test Acc: 31.20% | F1-Score: 0.31\n"
     ]
    }
   ],
   "source": [
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(X_train.shape[1], X_train.shape[1]),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(X_train.shape[1], 5504),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(5504, y_train.shape[1]),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "emo_model  = EmotionModel().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=emo_model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# Training Loop\n",
    "torch.manual_seed(SEED)\n",
    "torch.mps.manual_seed(SEED)\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    ### Training\n",
    "    emo_model.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits = emo_model(X_train)\n",
    "    y_pred = torch.softmax(y_logits, dim=1)\n",
    "    y_pred_idx = y_pred.argmax(dim=1)\n",
    "\n",
    "    # 2. Calculate loss and accuracy\n",
    "    y_train_idx = y_train.argmax(dim=1)\n",
    "    loss = loss_fn(y_logits, y_train_idx)\n",
    "    \n",
    "    # acc = multiclass_accuracy_fn(y_pred=y_pred, target=y_train_idx)\n",
    "    acc = accuracy_score(y_train_idx.detach().cpu().numpy(), y_pred_idx.detach().cpu().numpy()) \n",
    "    \n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Optimizer Step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    emo_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = emo_model(X_test).squeeze()\n",
    "        test_pred = torch.softmax(test_logits, dim=1)\n",
    "        test_pred_idx = test_pred.argmax(dim=1)\n",
    "        \n",
    "        # 2. Calculate the test loss/accuracy\n",
    "        y_test_idx = y_test.argmax(dim=1)\n",
    "        test_loss = loss_fn(test_logits, y_test_idx)\n",
    "        # test_acc = multiclass_accuracy_fn(y_pred=test_pred, target=y_test_idx)\n",
    "        test_acc = accuracy_score(y_test_idx.detach().cpu().numpy(), test_pred_idx.detach().cpu().numpy())\n",
    "\n",
    "    # Calculate f1 Score\n",
    "    test_f1_score = f1_score(y_test_idx.detach().cpu().numpy(), test_pred_idx.detach().cpu().numpy(), average='micro')\n",
    "    \n",
    "    # Print epoch output\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch} | Loss: {loss:.5f}%, Acc: {acc*100:.2f}% | Test Loss: {test_loss:.5f}%, Test Acc: {test_acc*100:.2f}% | F1-Score: {test_f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc1f976f-9256-4102-be59-5b639f876d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss: 2.07754%, Acc: 10.50% | Test Loss: 2.07661%, Test Acc: 11.35% | F1-Score: 0.11\n",
      "Epoch: 20 | Loss: 2.06514%, Acc: 22.26% | Test Loss: 2.06398%, Test Acc: 22.72% | F1-Score: 0.23\n",
      "Epoch: 30 | Loss: 2.05335%, Acc: 22.60% | Test Loss: 2.05223%, Test Acc: 22.60% | F1-Score: 0.23\n",
      "Epoch: 40 | Loss: 2.04307%, Acc: 22.60% | Test Loss: 2.04192%, Test Acc: 22.62% | F1-Score: 0.23\n",
      "Epoch: 50 | Loss: 2.03357%, Acc: 22.60% | Test Loss: 2.03234%, Test Acc: 22.62% | F1-Score: 0.23\n",
      "Epoch: 60 | Loss: 2.02449%, Acc: 22.60% | Test Loss: 2.02318%, Test Acc: 22.62% | F1-Score: 0.23\n",
      "Epoch: 70 | Loss: 2.01583%, Acc: 22.60% | Test Loss: 2.01443%, Test Acc: 22.62% | F1-Score: 0.23\n",
      "Epoch: 80 | Loss: 2.00774%, Acc: 22.60% | Test Loss: 2.00626%, Test Acc: 22.62% | F1-Score: 0.23\n",
      "Epoch: 90 | Loss: 2.00044%, Acc: 22.60% | Test Loss: 1.99890%, Test Acc: 22.62% | F1-Score: 0.23\n",
      "Epoch: 100 | Loss: 1.99415%, Acc: 22.60% | Test Loss: 1.99257%, Test Acc: 22.62% | F1-Score: 0.23\n"
     ]
    }
   ],
   "source": [
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(X_train.shape[1], X_train.shape[1]),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(X_train.shape[1], 5504),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(5504, 8000),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(8000, 5504),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(5504, 2752),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(2752, 1000),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(1000, 2752),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(2752, 5504),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(5504, y_train.shape[1]),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "emo_model_T2  = EmotionModel().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=emo_model_T2.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# Training Loop\n",
    "torch.manual_seed(SEED)\n",
    "torch.mps.manual_seed(SEED)\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    ### Training\n",
    "    emo_model_T2.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits = emo_model_T2(X_train)\n",
    "    y_pred = torch.softmax(y_logits, dim=1)\n",
    "\n",
    "    # 2. Calculate loss and accuracy\n",
    "    y_train_idx = y_train.argmax(dim=1)\n",
    "    loss = loss_fn(y_logits, y_train_idx)\n",
    "    \n",
    "    acc = multiclass_accuracy_fn(y_pred=y_pred, target=y_train_idx)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Optimizer Step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    emo_model_T2.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = emo_model_T2(X_test).squeeze()\n",
    "        test_pred = torch.softmax(test_logits, dim=1)\n",
    "\n",
    "        # 2. Calculate the test loss/accuracy\n",
    "        y_test_idx = y_test.argmax(dim=1)\n",
    "        test_loss = loss_fn(test_logits, y_test_idx)\n",
    "        test_acc = multiclass_accuracy_fn(y_pred=test_pred, target=y_test_idx)\n",
    "\n",
    "    # Print epoch output\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch} | Loss: {loss:.5f}%, Acc: {acc*100:.2f}% | Test Loss: {test_loss:.5f}%, Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cc553a4-7249-42b3-ab12-42db59059e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_accuracy_fn(y_pred, target):\n",
    "    winners = y_pred.argmax(dim=1)\n",
    "    # print(\"winners shape:\", winners.shape)\n",
    "    # print(\"target shape:\", target.shape)\n",
    "    if target.ndim == 2:\n",
    "        target_idx = target.argmax(dim=1)\n",
    "    else:\n",
    "        target_idx = target\n",
    "    \n",
    "    corrects = (winners == target_idx)\n",
    "    # print(\"corrects:\", corrects)\n",
    "    # print(\"acc:\", corrects.float().mean())\n",
    "    \n",
    "    # accuracy = corrects.sum().float() / float( target_idx.size(0) )\n",
    "    accuracy = corrects.float().mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6028fe6d-eaaf-4ddf-adce-d39cac5d4869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(X_train.shape[1], X_train.shape[1]),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(X_train.shape[1], 5504),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(5504, y_train.shape[1]),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "emo_model = EmotionModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61c3d81d-1341-42af-9d63-3f197b772f3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emo_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m loss_fn = nn.BCEWithLogitsLoss().to(device)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# optimizer = torch.optim.Adam(emo_model.parameters(), lr=1e-3)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m optimizer = torch.optim.SGD(params=\u001b[43memo_model\u001b[49m.parameters(), lr=\u001b[32m0.0001\u001b[39m, momentum=\u001b[32m0.9\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'emo_model' is not defined"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "# optimizer = torch.optim.Adam(emo_model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.SGD(params=emo_model.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acc11287-d661-4ccb-9d93-1691d9b24cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    # correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    # acc = (correct/len(y_pred)) * 100\n",
    "    print(y_true.shape, y_pred.shape)\n",
    "    print(\"y_true.dtype=\",y_true.dtype)\n",
    "    print(\"y_pred.dtype=\",y_pred.dtype)\n",
    "    eq_value = torch.eq(y_true, y_pred)\n",
    "    # print(\"eq_value.dtype=\", eq_value.dtype)\n",
    "    # print(\"eq_value=\", eq_value)\n",
    "    acc = (y_true == y_pred).float().mean().item() * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f6b1a8b-23d4-4dfc-97cc-7432e66455ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19661, 8]) torch.Size([19661, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([4916, 8]) torch.Size([4916, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([19661, 8]) torch.Size([19661, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([4916, 8]) torch.Size([4916, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([19661, 8]) torch.Size([19661, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([4916, 8]) torch.Size([4916, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([19661, 8]) torch.Size([19661, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([4916, 8]) torch.Size([4916, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([19661, 8]) torch.Size([19661, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([4916, 8]) torch.Size([4916, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([19661, 8]) torch.Size([19661, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([4916, 8]) torch.Size([4916, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([19661, 8]) torch.Size([19661, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([4916, 8]) torch.Size([4916, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([19661, 8]) torch.Size([19661, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([4916, 8]) torch.Size([4916, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([19661, 8]) torch.Size([19661, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([4916, 8]) torch.Size([4916, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([19661, 8]) torch.Size([19661, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([4916, 8]) torch.Size([4916, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "Epoch: 10 | Loss: 2.45119, Acc: 79.61% | Test Loss: 2.03165, Test Acc: 82.08\n",
      "torch.Size([19661, 8]) torch.Size([19661, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n",
      "torch.Size([4916, 8]) torch.Size([4916, 8])\n",
      "y_true.dtype= torch.float32\n",
      "y_pred.dtype= torch.float32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# 2. Calculate the test loss/accuracy\u001b[39;00m\n\u001b[32m     41\u001b[39m     test_loss = loss_fn(test_logits, y_test)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     test_acc = \u001b[43maccuracy_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Print epoch output\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36maccuracy_fn\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m      7\u001b[39m eq_value = torch.eq(y_true, y_pred)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# print(\"eq_value.dtype=\", eq_value.dtype)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# print(\"eq_value=\", eq_value)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m acc = \u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * \u001b[32m100\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m acc\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "torch.manual_seed(SEED)\n",
    "torch.mps.manual_seed(SEED)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    ### Training\n",
    "    emo_model.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits = emo_model(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "    # 2. Calculate loss and accuracy\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Optimizer Step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    emo_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = emo_model(X_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        # 2. Calculate the test loss/accuracy\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "    # Print epoch output\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d682c-86b3-4d59-a5c2-4f5dbdbb16c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a7f8c21-fdfc-4002-b92d-59c022875265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19661, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6918f4-4891-4c1d-9fcd-3a562ec6d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs=100\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "    # 1. Forward pass\n",
    "    # 2. Calculate the loss\n",
    "    # 3. Optimize the zero grad\n",
    "    # 4. Backpropagation\n",
    "    # 5. Optimizer Step (Gradient Descent)\n",
    "with torch.inference_mode():\n",
    "    y_logits = emo_model(X_train.to(device))[:5]\n",
    "    # print(y_logits)\n",
    "    y_pred_probs = torch.sigmoid(y_logits)\n",
    "    # print(y_pred_probs)\n",
    "    y_preds = torch.round(y_pred_probs)\n",
    "    y_preds_labels = torch.round(torch.sigmoid(emo_model(X_train.to(device))[:5]))\n",
    "    print(torch.eq(y_preds.squeeze(), y_preds_labels.squeeze()))\n",
    "    print(y_preds.squeeze())    \n",
    "    \n",
    "    #     f1 = f1_score(y_true_np, y_pred_np, average='macro')\n",
    "    #     f1_loss = 1.0 - f1\n",
    "\n",
    "    # print(\n",
    "    #     f\"Epoch {epoch+1:03d}/{epochs} | \"\n",
    "    #     f\"loss: {loss.item():.4f} | \"\n",
    "    #     f\"acc: {acc:.4f} | \"\n",
    "    #     f\"f1: {f1:.4f} | \"\n",
    "    #     f\"f1_loss: {f1_loss:.4f}\"\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dea6c667-f035-4dbf-a60c-ce2320a9a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float32))\n",
    "        self.bias = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.weights * x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3997de3e-d5fe-464b-802d-cd9273cd6385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.3904], requires_grad=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop example\n",
    "torch.manual_seed(SEED)\n",
    "model_0 = LinearRegressionModel()\n",
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4867e310-1da8-4bef-866a-8c993cb2840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Optimizer and Loss Functions\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb374fc-a5d7-4fca-b9d6-b415d1c5ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_0.train()\n",
    "    \n",
    "    y_pred = model_0(X_train)\n",
    "\n",
    "    print(y_pred.shape, y_train.shape)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    # model_0.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
