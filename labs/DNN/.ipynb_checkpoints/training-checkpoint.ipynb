{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ef2f36-ff33-4044-ab55-8ede356d1b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0.dev20251122'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Check pytorch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9a14b8-36f9-4449-897e-5075abba7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b200124-fa01-498b-b102-3b1fda0c1ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True mps\n"
     ]
    }
   ],
   "source": [
    "# Check and determine device type\n",
    "if torch.cuda.is_available():\n",
    "    gpuAvailable = True\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    gpuAvailable = True\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    gpuAvailable = False\n",
    "    device = torch.device(\"cpu\")\n",
    "print(gpuAvailable, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ad2d8f7-a7c5-4aef-8b3c-f1009a18350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset up\n",
    "curdir = os.getcwd()\n",
    "X = torch.load(os.path.join(curdir, 'model1_input.pt')).to(torch.float32)\n",
    "y = torch.tensor(np.loadtxt(os.path.join(curdir, 'emotion_targets.csv'), delimiter=','), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ba51a9-dd6d-47da-865d-cf06bb2726be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset variables\n",
    "train_slice = .60\n",
    "test_slice = .20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f405dd-1c46-43bd-aca7-8e72d96a95b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training, testing, and validation sets\n",
    "# skf = StratifiedKFold(n_splits=a3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_slice, random_state=SEED, shuffle=True)\n",
    "# validation_slice = X_test.shape[0]/X_train.shape[0]\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_slice, random_state=SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3125aaee-7c6a-4da5-a074-2706c977c72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19661, 11008]) torch.Size([19661, 8])\n",
      "torch.Size([4916, 11008]) torch.Size([4916, 8])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6028fe6d-eaaf-4ddf-adce-d39cac5d4869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(X_train.shape[1], X_train.shape[1]),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(X_train.shape[1], 5504),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(5504, y_train.shape[1]),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "emo_model = EmotionModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61c3d81d-1341-42af-9d63-3f197b772f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "# optimizer = torch.optim.Adam(emo_model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.SGD(params=emo_model.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acc11287-d661-4ccb-9d93-1691d9b24cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    # correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    # acc = (correct/len(y_pred)) * 100\n",
    "    acc = (y_true == y_pred).float().mean().item() * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f6b1a8b-23d4-4dfc-97cc-7432e66455ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss: 0.30189, Acc: 88.18% | Test Loss: 0.30987, Test Acc: 87.90\n",
      "Epoch: 20 | Loss: 0.30038, Acc: 88.19% | Test Loss: 0.30882, Test Acc: 87.90\n",
      "Epoch: 30 | Loss: 0.29902, Acc: 88.24% | Test Loss: 0.30788, Test Acc: 87.90\n",
      "Epoch: 40 | Loss: 0.29781, Acc: 88.26% | Test Loss: 0.30709, Test Acc: 87.92\n",
      "Epoch: 50 | Loss: 0.29673, Acc: 88.29% | Test Loss: 0.30636, Test Acc: 87.92\n",
      "Epoch: 60 | Loss: 0.29573, Acc: 88.30% | Test Loss: 0.30571, Test Acc: 87.95\n",
      "Epoch: 70 | Loss: 0.29480, Acc: 88.32% | Test Loss: 0.30513, Test Acc: 87.97\n",
      "Epoch: 80 | Loss: 0.29394, Acc: 88.34% | Test Loss: 0.30459, Test Acc: 87.99\n",
      "Epoch: 90 | Loss: 0.29314, Acc: 88.36% | Test Loss: 0.30410, Test Acc: 87.99\n",
      "Epoch: 100 | Loss: 0.29239, Acc: 88.38% | Test Loss: 0.30365, Test Acc: 88.01\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "torch.manual_seed(SEED)\n",
    "torch.mps.manual_seed(SEED)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    ### Training\n",
    "    emo_model.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits = emo_model(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "    # 2. Calculate loss and accuracy\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Optimizer Step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    emo_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = emo_model(X_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        # 2. Calculate the test loss/accuracy\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "    # Print epoch output\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec523e2d-1b3c-4c54-a083-077774f85a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss: 3.80424, Acc: 88.38% | Test Loss: 7.50318, Test Acc: 73.29\n",
      "Epoch: 20 | Loss: 1.23097, Acc: 88.38% | Test Loss: 0.88740, Test Acc: 88.01\n",
      "Epoch: 30 | Loss: 0.41538, Acc: 88.38% | Test Loss: 0.44557, Test Acc: 82.64\n",
      "Epoch: 40 | Loss: 0.36070, Acc: 88.38% | Test Loss: 0.35920, Test Acc: 88.00\n",
      "Epoch: 50 | Loss: 0.33686, Acc: 88.38% | Test Loss: 0.33363, Test Acc: 88.00\n",
      "Epoch: 60 | Loss: 0.32648, Acc: 88.38% | Test Loss: 0.32348, Test Acc: 88.01\n",
      "Epoch: 70 | Loss: 0.31767, Acc: 88.38% | Test Loss: 0.31956, Test Acc: 88.01\n",
      "Epoch: 80 | Loss: 0.31419, Acc: 88.38% | Test Loss: 0.31609, Test Acc: 88.00\n",
      "Epoch: 90 | Loss: 0.31194, Acc: 88.38% | Test Loss: 0.31365, Test Acc: 88.01\n",
      "Epoch: 100 | Loss: 0.31023, Acc: 88.38% | Test Loss: 0.31207, Test Acc: 88.03\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "emo_model_T2 = EmotionModel().to(device)\n",
    "\n",
    "optimizer_T2 = torch.optim.SGD(params=emo_model_T2.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training Loop\n",
    "torch.manual_seed(SEED)\n",
    "torch.mps.manual_seed(SEED)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    ### Training\n",
    "    emo_model_T2.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits_T2 = emo_model_T2(X_train).squeeze()\n",
    "    y_pred_T2 = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "    # 2. Calculate loss and accuracy\n",
    "    loss_T2 = loss_fn(y_logits_T2, y_train)\n",
    "    acc_T2 = accuracy_fn(y_true=y_train, y_pred=y_pred_T2)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer_T2.zero_grad()\n",
    "    \n",
    "    # 4. Backpropagation\n",
    "    loss_T2.backward()\n",
    "    \n",
    "    # 5. Optimizer Step\n",
    "    optimizer_T2.step()\n",
    "\n",
    "    ### Testing\n",
    "    emo_model_T2.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits_T2 = emo_model_T2(X_test).squeeze()\n",
    "        test_pred_T2 = torch.round(torch.sigmoid(test_logits_T2))\n",
    "\n",
    "        # 2. Calculate the test loss/accuracy\n",
    "        test_loss_T2 = loss_fn(test_logits_T2, y_test)\n",
    "        test_acc_T2 = accuracy_fn(y_true=y_test, y_pred=test_pred_T2)\n",
    "\n",
    "    # Print epoch output\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch} | Loss: {loss_T2:.5f}, Acc: {acc_T2:.2f}% | Test Loss: {test_loss_T2:.5f}, Test Acc: {test_acc_T2:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd44d2f8-8209-4bff-8630-a166420b056d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (19661) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# 2. Calculate loss and accuracy\u001b[39;00m\n\u001b[32m     37\u001b[39m loss_T3 = loss_fn_T3(y_logits_T3, y_train)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m acc_T3 = \u001b[43maccuracy_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_pred_T3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# 3. Optimizer zero grad\u001b[39;00m\n\u001b[32m     41\u001b[39m optimizer_T3.zero_grad()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36maccuracy_fn\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maccuracy_fn\u001b[39m(y_true, y_pred):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     correct = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43meq\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m.sum().item()\n\u001b[32m      3\u001b[39m     acc = (correct/\u001b[38;5;28mlen\u001b[39m(y_pred)) * \u001b[32m100\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# acc = (y_true == y_pred).float().mean().item() * 100\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (8) must match the size of tensor b (19661) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "class EmotionModel_T3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(X_train.shape[1], X_train.shape[1]),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(X_train.shape[1], 5504),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(5504, y_train.shape[1]),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "emo_model_T3 = EmotionModel_T3().to(device)\n",
    "\n",
    "loss_fn_T3 = nn.CrossEntropyLoss()\n",
    "optimizer_T3 = torch.optim.SGD(params=emo_model_T3.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# Training Loop\n",
    "torch.manual_seed(SEED)\n",
    "torch.mps.manual_seed(SEED)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    ### Training\n",
    "    emo_model_T3.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits_T3 = emo_model_T3(X_train)\n",
    "    y_pred_T3 = torch.softmax(y_logits_T3, dim=1).argmax(dim=1)\n",
    "\n",
    "    # 2. Calculate loss and accuracy\n",
    "    loss_T3 = loss_fn_T3(y_logits_T3, y_train)\n",
    "    acc_T3 = accuracy_fn(y_true=y_train, y_pred=y_pred_T3)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer_T3.zero_grad()\n",
    "    \n",
    "    # 4. Backpropagation\n",
    "    loss_T3.backward()\n",
    "    \n",
    "    # 5. Optimizer Step\n",
    "    optimizer_T3.step()\n",
    "\n",
    "    ### Testing\n",
    "    emo_model_T3.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits_T3 = emo_model_T3(X_test).squeeze()\n",
    "        test_pred_T3 = torch.softmax(test_logits_T3, dim=1).argmax(dim=1)\n",
    "\n",
    "        # 2. Calculate the test loss/accuracy\n",
    "        test_loss_T3 = loss_fn(test_logits_T3, y_test)\n",
    "        test_acc_T3 = accuracy_fn(y_true=y_test, y_pred=test_pred_T3)\n",
    "\n",
    "    # Print epoch output\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch} | Loss: {loss_T3:.5f}, Acc: {acc_T3:.2f}% | Test Loss: {test_loss_T3:.5f}, Test Acc: {test_acc_T3:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6918f4-4891-4c1d-9fcd-3a562ec6d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs=100\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "    # 1. Forward pass\n",
    "    # 2. Calculate the loss\n",
    "    # 3. Optimize the zero grad\n",
    "    # 4. Backpropagation\n",
    "    # 5. Optimizer Step (Gradient Descent)\n",
    "with torch.inference_mode():\n",
    "    y_logits = emo_model(X_train.to(device))[:5]\n",
    "    # print(y_logits)\n",
    "    y_pred_probs = torch.sigmoid(y_logits)\n",
    "    # print(y_pred_probs)\n",
    "    y_preds = torch.round(y_pred_probs)\n",
    "    y_preds_labels = torch.round(torch.sigmoid(emo_model(X_train.to(device))[:5]))\n",
    "    print(torch.eq(y_preds.squeeze(), y_preds_labels.squeeze()))\n",
    "    print(y_preds.squeeze())    \n",
    "    \n",
    "    #     f1 = f1_score(y_true_np, y_pred_np, average='macro')\n",
    "    #     f1_loss = 1.0 - f1\n",
    "\n",
    "    # print(\n",
    "    #     f\"Epoch {epoch+1:03d}/{epochs} | \"\n",
    "    #     f\"loss: {loss.item():.4f} | \"\n",
    "    #     f\"acc: {acc:.4f} | \"\n",
    "    #     f\"f1: {f1:.4f} | \"\n",
    "    #     f\"f1_loss: {f1_loss:.4f}\"\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dea6c667-f035-4dbf-a60c-ce2320a9a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float32))\n",
    "        self.bias = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.weights * x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3997de3e-d5fe-464b-802d-cd9273cd6385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.3904], requires_grad=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop example\n",
    "torch.manual_seed(SEED)\n",
    "model_0 = LinearRegressionModel()\n",
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4867e310-1da8-4bef-866a-8c993cb2840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Optimizer and Loss Functions\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb374fc-a5d7-4fca-b9d6-b415d1c5ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_0.train()\n",
    "    \n",
    "    y_pred = model_0(X_train)\n",
    "\n",
    "    print(y_pred.shape, y_train.shape)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    # model_0.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tone AI",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
