{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ef2f36-ff33-4044-ab55-8ede356d1b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Check pytorch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9a14b8-36f9-4449-897e-5075abba7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b200124-fa01-498b-b102-3b1fda0c1ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False cpu\n"
     ]
    }
   ],
   "source": [
    "# Check and determine device type\n",
    "if torch.cuda.is_available():\n",
    "    gpuAvailable = True\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    gpuAvailable = True\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    gpuAvailable = False\n",
    "    device = torch.device(\"cpu\")\n",
    "print(gpuAvailable, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ad2d8f7-a7c5-4aef-8b3c-f1009a18350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset up\n",
    "curdir = os.getcwd()\n",
    "X = torch.load(os.path.join(curdir, 'model1_input.pt')).to(torch.float32)\n",
    "y = torch.tensor(np.loadtxt(os.path.join(curdir, 'emotion_targets.csv'), delimiter=','), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ba51a9-dd6d-47da-865d-cf06bb2726be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset variables\n",
    "train_slice = .80\n",
    "test_slice = .20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f405dd-1c46-43bd-aca7-8e72d96a95b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training, testing, and validation sets\n",
    "# skf = StratifiedKFold(n_splits=a3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_slice, random_state=SEED, shuffle=True)\n",
    "# validation_slice = X_test.shape[0]/X_train.shape[0]\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_slice, random_state=SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3125aaee-7c6a-4da5-a074-2706c977c72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19661, 11008]) torch.Size([19661, 8])\n",
      "torch.Size([4916, 11008]) torch.Size([4916, 8])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6028fe6d-eaaf-4ddf-adce-d39cac5d4869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(X_train.shape[1], X_train.shape[1]),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(X_train.shape[1], 5504),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(5504, y_train.shape[1]),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "emo_model = EmotionModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61c3d81d-1341-42af-9d63-3f197b772f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "# optimizer = torch.optim.Adam(emo_model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.SGD(params=emo_model.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acc11287-d661-4ccb-9d93-1691d9b24cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    # correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    # acc = (correct/len(y_pred)) * 100\n",
    "    print(y_true.shape, y_pred.shape)\n",
    "    print(\"y_true.dtype=\",y_true.dtype)\n",
    "    print(\"y_pred.dtype=\",y_pred.dtype)\n",
    "    eq_value = torch.eq(y_true, y_pred)\n",
    "    # print(\"eq_value.dtype=\", eq_value.dtype)\n",
    "    # print(\"eq_value=\", eq_value)\n",
    "    acc = (y_true == y_pred).float().mean().item() * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cc553a4-7249-42b3-ab12-42db59059e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_accuracy_fn(y_pred, target):\n",
    "    winners = y_pred.argmax(dim=1)\n",
    "    print(winners.shape)\n",
    "    \n",
    "    if target.ndim == 2:\n",
    "        target_idx = target.argmax(dim=1)\n",
    "    else:\n",
    "        target_idx = target\n",
    "    \n",
    "    corrects = (winners == target)\n",
    "    print(corrects.sum().float(), \"/\", float( target.size(0) ))\n",
    "    \n",
    "    accuracy = corrects.sum().float() / float( target.size(0) )\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d0f7d3b-8d9e-492f-b6bf-c0a99203d411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19661, 11008]) torch.Size([19661, 8])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd44d2f8-8209-4bff-8630-a166420b056d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 17.59 GiB, other allocations: 2.70 MiB, max allowed: 18.13 GiB). Tried to allocate 825.61 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m emo_model_T3.train()\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# 1. Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m y_logits_T3 = \u001b[43memo_model_T3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m y_pred_T3 = torch.softmax(y_logits_T3, dim=\u001b[32m1\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# print(\"y_pred_T3.shape=\", y_pred_T3.shape)\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# print(\"y_train.shape=\", y_train.shape)\u001b[39;00m\n\u001b[32m     41\u001b[39m \n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# 2. Calculate loss and accuracy\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/ToneAI/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/ToneAI/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1789\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1787\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mEmotionModel_T3.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/ToneAI/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/ToneAI/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1789\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1787\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/ToneAI/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:253\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/ToneAI/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/ToneAI/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1789\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1787\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/ToneAI/.venv/lib/python3.13/site-packages/torch/nn/modules/activation.py:922\u001b[39m, in \u001b[36mLeakyReLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    919\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    920\u001b[39m \u001b[33;03m    Run forward pass.\u001b[39;00m\n\u001b[32m    921\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mleaky_relu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnegative_slope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/ToneAI/.venv/lib/python3.13/site-packages/torch/nn/functional.py:1919\u001b[39m, in \u001b[36mleaky_relu\u001b[39m\u001b[34m(input, negative_slope, inplace)\u001b[39m\n\u001b[32m   1917\u001b[39m     result = torch._C._nn.leaky_relu_(\u001b[38;5;28minput\u001b[39m, negative_slope)\n\u001b[32m   1918\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1919\u001b[39m     result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mleaky_relu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_slope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 17.59 GiB, other allocations: 2.70 MiB, max allowed: 18.13 GiB). Tried to allocate 825.61 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "class EmotionModel_T3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(X_train.shape[1], X_train.shape[1]),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(X_train.shape[1], 5504),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(5504, y_train.shape[1]),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "emo_model_T3 = EmotionModel_T3().to(device)\n",
    "\n",
    "loss_fn_T3 = nn.CrossEntropyLoss()\n",
    "optimizer_T3 = torch.optim.SGD(params=emo_model_T3.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# Training Loop\n",
    "torch.manual_seed(SEED)\n",
    "torch.mps.manual_seed(SEED)\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    ### Training\n",
    "    emo_model_T3.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits_T3 = emo_model_T3(X_train)\n",
    "    y_pred_T3 = torch.softmax(y_logits_T3, dim=1)\n",
    "    # print(\"y_pred_T3.shape=\", y_pred_T3.shape)\n",
    "    # print(\"y_train.shape=\", y_train.shape)\n",
    "\n",
    "    # 2. Calculate loss and accuracy\n",
    "    y_train_idx = y_train.argmax(dim=1)\n",
    "    loss_T3 = loss_fn_T3(y_logits_T3, y_train_idx)\n",
    "    \n",
    "    acc_T3 = multiclass_accuracy_fn(y_pred=y_pred_T3, target=y_train)\n",
    "    print(\"train acc:\", acc_T3)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer_T3.zero_grad()\n",
    "    \n",
    "    # 4. Backpropagation\n",
    "    loss_T3.backward()\n",
    "    \n",
    "    # 5. Optimizer Step\n",
    "    optimizer_T3.step()\n",
    "\n",
    "    ### Testing\n",
    "    emo_model_T3.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits_T3 = emo_model_T3(X_test).squeeze()\n",
    "        test_pred_T3 = torch.softmax(test_logits_T3, dim=1)\n",
    "\n",
    "        # 2. Calculate the test loss/accuracy\n",
    "        y_test_idx = y_test.argmax(dim=1)\n",
    "        test_loss_T3 = loss_fn(test_logits_T3, y_test_idx)\n",
    "        test_acc_T3 = multiclass_accuracy_fn(y_pred=test_pred_T3, target=y_test)\n",
    "\n",
    "    # Print epoch output\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch} | Loss: {loss_T3:.5f}, Acc: {acc_T3:.2f}% | Test Loss: {test_loss_T3:.5f}, Test Acc: {test_acc_T3:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f6b1a8b-23d4-4dfc-97cc-7432e66455ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss: 0.30189, Acc: 88.18% | Test Loss: 0.30987, Test Acc: 87.90\n",
      "Epoch: 20 | Loss: 0.30038, Acc: 88.19% | Test Loss: 0.30882, Test Acc: 87.90\n",
      "Epoch: 30 | Loss: 0.29902, Acc: 88.24% | Test Loss: 0.30788, Test Acc: 87.90\n",
      "Epoch: 40 | Loss: 0.29781, Acc: 88.26% | Test Loss: 0.30709, Test Acc: 87.92\n",
      "Epoch: 50 | Loss: 0.29673, Acc: 88.29% | Test Loss: 0.30636, Test Acc: 87.92\n",
      "Epoch: 60 | Loss: 0.29573, Acc: 88.30% | Test Loss: 0.30571, Test Acc: 87.95\n",
      "Epoch: 70 | Loss: 0.29480, Acc: 88.32% | Test Loss: 0.30513, Test Acc: 87.97\n",
      "Epoch: 80 | Loss: 0.29394, Acc: 88.34% | Test Loss: 0.30459, Test Acc: 87.99\n",
      "Epoch: 90 | Loss: 0.29314, Acc: 88.36% | Test Loss: 0.30410, Test Acc: 87.99\n",
      "Epoch: 100 | Loss: 0.29239, Acc: 88.38% | Test Loss: 0.30365, Test Acc: 88.01\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "torch.manual_seed(SEED)\n",
    "torch.mps.manual_seed(SEED)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    ### Training\n",
    "    emo_model.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits = emo_model(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "    # 2. Calculate loss and accuracy\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Optimizer Step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    emo_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = emo_model(X_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        # 2. Calculate the test loss/accuracy\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "    # Print epoch output\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "883d682c-86b3-4d59-a5c2-4f5dbdbb16c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19661, 8])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a7f8c21-fdfc-4002-b92d-59c022875265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19661, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6918f4-4891-4c1d-9fcd-3a562ec6d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs=100\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "    # 1. Forward pass\n",
    "    # 2. Calculate the loss\n",
    "    # 3. Optimize the zero grad\n",
    "    # 4. Backpropagation\n",
    "    # 5. Optimizer Step (Gradient Descent)\n",
    "with torch.inference_mode():\n",
    "    y_logits = emo_model(X_train.to(device))[:5]\n",
    "    # print(y_logits)\n",
    "    y_pred_probs = torch.sigmoid(y_logits)\n",
    "    # print(y_pred_probs)\n",
    "    y_preds = torch.round(y_pred_probs)\n",
    "    y_preds_labels = torch.round(torch.sigmoid(emo_model(X_train.to(device))[:5]))\n",
    "    print(torch.eq(y_preds.squeeze(), y_preds_labels.squeeze()))\n",
    "    print(y_preds.squeeze())    \n",
    "    \n",
    "    #     f1 = f1_score(y_true_np, y_pred_np, average='macro')\n",
    "    #     f1_loss = 1.0 - f1\n",
    "\n",
    "    # print(\n",
    "    #     f\"Epoch {epoch+1:03d}/{epochs} | \"\n",
    "    #     f\"loss: {loss.item():.4f} | \"\n",
    "    #     f\"acc: {acc:.4f} | \"\n",
    "    #     f\"f1: {f1:.4f} | \"\n",
    "    #     f\"f1_loss: {f1_loss:.4f}\"\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dea6c667-f035-4dbf-a60c-ce2320a9a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float32))\n",
    "        self.bias = nn.Parameter(torch.rand(1, requires_grad=True, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.weights * x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3997de3e-d5fe-464b-802d-cd9273cd6385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.3904], requires_grad=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop example\n",
    "torch.manual_seed(SEED)\n",
    "model_0 = LinearRegressionModel()\n",
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4867e310-1da8-4bef-866a-8c993cb2840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Optimizer and Loss Functions\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb374fc-a5d7-4fca-b9d6-b415d1c5ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_0.train()\n",
    "    \n",
    "    y_pred = model_0(X_train)\n",
    "\n",
    "    print(y_pred.shape, y_train.shape)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    # model_0.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
